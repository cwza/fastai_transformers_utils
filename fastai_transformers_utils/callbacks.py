# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_callbacks.ipynb (unless otherwise specified).

__all__ = ['GPT2LMHeadCallback', 'BertSeqClassificationCallback']

# Cell
from fastcore.all import *
from fastai2.basics import *

# Cell
class GPT2LMHeadCallback(Callback):
    def after_pred(self):
        ''' The output of AutoModelWithLMHead is (last_hidden_state, past)
            What fastai want is last_hidden_state '''
        last_hidden_state = self.learn.pred[0]
        self.learn.pred = last_hidden_state

# Cell
class BertSeqClassificationCallback(Callback):
    ''' It should be ok to use it in all Bert like model. eg: Roberta
    '''
    def __init__(self, pad_id: int):
        self.pad_id = pad_id

    def begin_batch(self):
        ''' Instead of input_ids, we need to pass the attention_mask to AutoModelForSequenceClassification to avoid it to attention to padding tokens.
        '''
        input_ids = self.learn.xb[0]
        device = input_ids.device
        attention_mask = torch.where(input_ids == self.pad_id, torch.tensor(0, device=device), torch.tensor(1, device=device)).to(device)
        self.learn.xb = [input_ids, attention_mask]

    def after_pred(self):
        ''' The output of AutoModelForSequenceClassification is (logits, )
            What fastai want is logits '''
        logits = self.learn.pred[0]
        self.learn.pred = logits