# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_tokenizers.ipynb (unless otherwise specified).

__all__ = ['TransformersTokenizer', 'GPT2DecoderTokenizer']

# Cell
# export
from typing import *
from fastai2.basics import *

from transformers import GPT2Tokenizer, PreTrainedTokenizer, AutoTokenizer

# Cell
class TransformersTokenizer():
    ''' fastai want the tokenizer can handle list of string.
        use in parallel_gen() '''
    def __init__(self, tokenizer: PreTrainedTokenizer):
        self.tokenizer = tokenizer
    def __call__(self, items: List[str]):
        return map(self.tokenizer.tokenize, items)

# Cell
class GPT2DecoderTokenizer(GPT2Tokenizer):
    '''
        Add special tokens: <|bos|>, <|pad|>.
        Add <|bos|> to the begin of the tokenized string and add <|endoftext|> to the end of the tokenized string.
        For the decoder of machine translation
    '''
    def __init__(self, *inputs, **kwargs):
        super().__init__(*inputs, **kwargs)
        special_tokens_map = dict(bos_token='<|bos|>', pad_token='<|pad|>')
        self.add_special_tokens(special_tokens_map)
    def _tokenize(self, text, add_prefix_space=False):
        bpe_tokens = super()._tokenize(text, add_prefix_space=add_prefix_space)
        return [self.bos_token] + bpe_tokens + [self.eos_token]