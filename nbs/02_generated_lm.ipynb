{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture\n",
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastai2.basics import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from transformers.modeling_utils import PreTrainedModel, top_k_top_p_filtering, BeamHypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp generated_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeneratedLM\n",
    "> Language Model or Decoder with Generate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FakeDecoder, FakeLM for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDecoder(nn.Module):\n",
    "    ''' with memory not support past'''\n",
    "    def __init__(self, tgt_vocab_size):\n",
    "        super().__init__()\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "    def forward(self, tgt, momory, **kwargs):\n",
    "        ''' \n",
    "        inputs: (tgt, memory)\n",
    "            tgt: (b, tgt_seq_len)\n",
    "            memory: (b, src_seq_len, embed_dim)\n",
    "        returns: logits, others\n",
    "            logits: (b, tgt_seq_len, tgt_vocab_size)\n",
    "        '''\n",
    "        logits = torch.randn((*tgt.shape, self.tgt_vocab_size))\n",
    "        return logits, None\n",
    "    \n",
    "class FakeLM(nn.Module):\n",
    "    ''' without memory support past '''\n",
    "    def __init__(self, tgt_vocab_size):\n",
    "        super().__init__()\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "    def forward(self, tgt, past=None, **kwargs):\n",
    "        ''' \n",
    "        if past==None:\n",
    "            inputs: (tgt)\n",
    "                tgt: (b, tgt_seq_len)\n",
    "            returns: logits, presents, others\n",
    "                logits: (b, tgt_seq_len, tgt_vocab_size)\n",
    "                presents: List of (2, b, ...)\n",
    "        else:\n",
    "            inputs: (tgt, past)\n",
    "                tgt: (b, 1)   \n",
    "                past: List of (2, bs, num_heads, tgt_seq_len-1, ..)\n",
    "            returns: logits, presents, others\n",
    "                logits: (b, tgt_seq_len, tgt_vocab_size)\n",
    "                presents: List of (2, bs, num_heads, tgt_seq_len, ..)\n",
    "        '''\n",
    "        if past is None:\n",
    "            b = tgt.shape[0]\n",
    "            tgt_seq_len = tgt.shape[1]\n",
    "            logits = torch.randn((b, tgt_seq_len, self.tgt_vocab_size))\n",
    "            presents = [torch.randn((2, b, 12, tgt_seq_len, 16))] * 6\n",
    "        else:\n",
    "            b = tgt.shape[0]\n",
    "            tgt_seq_len = past[0].shape[3]+1\n",
    "            logits = torch.randn((b, tgt_seq_len, self.tgt_vocab_size))\n",
    "            presents = [torch.randn((2, b, 12, tgt_seq_len, 16))] * 6\n",
    "        return logits, presents, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n",
    "tgt_seq_len = 10\n",
    "tgt_vocab_size = 20\n",
    "memory = torch.randn((bs, 9, 9))\n",
    "past = [torch.randn((2, bs, 12, tgt_seq_len-1, 16))] * 6\n",
    "pad_token_id=0\n",
    "eos_token_id=tgt_vocab_size-1\n",
    "bos_token_id=tgt_vocab_size-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = FakeDecoder(tgt_vocab_size)\n",
    "lm = FakeLM(tgt_vocab_size)\n",
    "\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "test_eq(decoder(tgt, memory)[0].shape, (bs, tgt_seq_len, tgt_vocab_size))\n",
    "test_eq(lm(tgt)[0].shape, (bs, tgt_seq_len, tgt_vocab_size))\n",
    "test_eq(lm(tgt)[1][0].shape, (2, bs, 12, tgt_seq_len, 16))\n",
    "\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "test_eq(lm(tgt, past)[0].shape, (bs, tgt_seq_len, tgt_vocab_size))\n",
    "test_eq(lm(tgt, past)[1][0].shape, (2, bs, 12, tgt_seq_len, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneratedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class GeneratedLM():\n",
    "    def __init__(self, lm, vocab_size, support_past=False):\n",
    "        '''\n",
    "        Your lm's forward function should be this format:\n",
    "        if support_past==False\n",
    "            inputs: (tgt, *model_otherargs, **model_otherkwargs)\n",
    "                tgt: (bs, tgt_seq_len)\n",
    "            returns: logits, others\n",
    "                logits: (bs, tgt_seq_len, tgt_vocab_size)\n",
    "        else:\n",
    "            inputs: (tgt, past, *model_otherargs, **model_otherkwargs)\n",
    "                tgt: (bs, 1)\n",
    "                past: The infos for quickly generate sentence.  List of (2, bs, num_heads, tgt_seq_len-1, ..)\n",
    "            returns: logits, presents, others\n",
    "                logits: (bs, tgt_seq_len, tgt_vocab_size)\n",
    "                presents: List of (2, bs, num_heads, tgt_seq_len, ..)\n",
    "        '''\n",
    "        self.lm = lm\n",
    "        self.vocab_size = vocab_size\n",
    "        self.support_past = support_past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _generate_no_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def _generate_no_beam_search(\n",
    "    self:GeneratedLM,\n",
    "    tgt, # (bs, tgt_seq_len)\n",
    "    max_length,\n",
    "    do_sample,\n",
    "    temperature,\n",
    "    top_k,\n",
    "    top_p,\n",
    "    repetition_penalty,\n",
    "    pad_token_id,\n",
    "    eos_token_ids,\n",
    "    model_otherargs=[],\n",
    "    model_otherkwargs={},\n",
    "):\n",
    "    \"\"\" Generate sequences for each example without beam search (num_beams == 1).\n",
    "        All returned sequence are generated independantly.\n",
    "    \"\"\"\n",
    "    # current position / max lengths / length of generated sentences / unfinished sentences\n",
    "    batch_size = tgt.shape[0]\n",
    "    cur_len = tgt.shape[1]\n",
    "    unfinished_sents = tgt.new(batch_size).fill_(1)\n",
    "\n",
    "    past = None\n",
    "\n",
    "    while cur_len < max_length:\n",
    "#         print('===================================:', cur_len)\n",
    "#         print(tgt)\n",
    "        \n",
    "        if self.support_past==False:\n",
    "            outputs = self.lm(tgt, *model_otherargs, **model_otherkwargs)\n",
    "        else:\n",
    "            model_inputs = tgt if past is None else tgt[:, -1].unsqueeze(-1)\n",
    "            outputs = self.lm(model_inputs, past, *model_otherargs, **model_otherkwargs) \n",
    "        next_token_logits = outputs[0][:, -1, :]\n",
    "        \n",
    "        # if model has past, then set the past variable to speed up decoding\n",
    "        if self.support_past:\n",
    "            past = outputs[1]\n",
    "\n",
    "        # repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)\n",
    "        if repetition_penalty != 1.0:\n",
    "            for i in range(batch_size):\n",
    "                for previous_token in set(tgt[i].tolist()):\n",
    "                    # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n",
    "                    if next_token_logits[i, previous_token] < 0:\n",
    "                        next_token_logits[i, previous_token] *= repetition_penalty\n",
    "                    else:\n",
    "                        next_token_logits[i, previous_token] /= repetition_penalty\n",
    "\n",
    "        if do_sample:\n",
    "            # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "            if temperature != 1.0:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "            # Top-p/top-k filtering\n",
    "            next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            # Sample\n",
    "            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1).squeeze(1)\n",
    "        else:\n",
    "            # Greedy decoding\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # update generations and finished sentences\n",
    "        tokens_to_add = next_token * unfinished_sents + pad_token_id * (1 - unfinished_sents)\n",
    "        tgt = torch.cat([tgt, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "        for eos_token_id in eos_token_ids:\n",
    "            unfinished_sents.mul_(tokens_to_add.ne(eos_token_id).long())\n",
    "        cur_len = cur_len + 1\n",
    "\n",
    "        # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
    "        if unfinished_sents.max() == 0:\n",
    "            break\n",
    "\n",
    "    # add eos_token_ids to unfinished sentences\n",
    "    if cur_len == max_length:\n",
    "        tgt[:, -1].masked_fill_(unfinished_sents.to(dtype=torch.bool), eos_token_ids[0])\n",
    "\n",
    "    return tgt # (bs, (tgt_seq_len <= ? <= max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "max_length=20\n",
    "generate_args = dict(   \n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_k=3,\n",
    "    top_p=0.5,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=pad_token_id,\n",
    "    eos_token_ids=[eos_token_id],\n",
    ")\n",
    "\n",
    "# With memory, Without past\n",
    "generated_decoder = GeneratedLM(decoder, tgt_vocab_size, support_past=False)\n",
    "tgt = torch.zeros((bs, 1), dtype=torch.long).fill_(bos_token_id)\n",
    "result = generated_decoder._generate_no_beam_search(tgt, **generate_args, model_otherargs=[memory])\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= 1\n",
    "\n",
    "# Without memory, With past\n",
    "generated_lm = GeneratedLM(lm, tgt_vocab_size, support_past=True)\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "result = generated_lm._generate_no_beam_search(tgt, **generate_args)\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= tgt_seq_len\n",
    "\n",
    "# Without memory, Without past\n",
    "generated_lm = GeneratedLM(lm, tgt_vocab_size, support_past=False)\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "result = generated_lm._generate_no_beam_search(tgt, **generate_args)\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= tgt_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _generate_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "def _generate_beam_search(\n",
    "    self:GeneratedLM,\n",
    "    tgt, # (b, tgt_seq_len)\n",
    "    max_length,\n",
    "    do_sample,\n",
    "    temperature,\n",
    "    top_k,\n",
    "    top_p,\n",
    "    repetition_penalty,\n",
    "    pad_token_id,\n",
    "    eos_token_ids,\n",
    "    length_penalty,\n",
    "    num_beams,\n",
    "    vocab_size,\n",
    "    model_otherargs=[],\n",
    "    model_otherkwargs={},\n",
    "):\n",
    "    \"\"\" Generate sequences for each example with beam search.\n",
    "    \"\"\"\n",
    "    batch_size = tgt.shape[0]\n",
    "    cur_len = tgt.shape[1]\n",
    "    # Expand input to num beams\n",
    "    tgt = tgt.unsqueeze(1).expand(batch_size, num_beams, cur_len)\n",
    "    tgt = tgt.contiguous().view(batch_size * num_beams, cur_len)  # (batch_size * num_beams, cur_len)\n",
    "\n",
    "    # generated hypotheses\n",
    "    generated_hyps = [\n",
    "        BeamHypotheses(num_beams, max_length, length_penalty, early_stopping=False) for _ in range(batch_size)\n",
    "    ]\n",
    "\n",
    "    # scores for each sentence in the beam\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=tgt.device)\n",
    "    beam_scores[:, 1:] = -1e9\n",
    "    beam_scores = beam_scores.view(-1)  # shape (batch_size * num_beams,)\n",
    "\n",
    "    # cache compute states\n",
    "    past = None\n",
    "\n",
    "    # done sentences\n",
    "    done = [False for _ in range(batch_size)]\n",
    "\n",
    "    while cur_len < max_length:\n",
    "        if self.support_past==False:\n",
    "            outputs = self.lm(tgt, *model_otherargs, **model_otherkwargs)\n",
    "        else:\n",
    "            model_inputs = tgt if past is None else tgt[:, -1].unsqueeze(-1)\n",
    "            outputs = self.lm(model_inputs, past, *model_otherargs, **model_otherkwargs) \n",
    "#         outputs = self.lm(tgt, *model_otherargs, **model_otherkwargs) if self.support_past==False else self.lm(tgt, past, *model_otherargs, **model_otherkwargs) # (batch_size * num_beams, cur_len, vocab_size)\n",
    "        scores = outputs[0][:, -1, :]  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "        # if model has past, then set the past variable to speed up decoding\n",
    "        if self.support_past:\n",
    "            past = outputs[1]\n",
    "\n",
    "        # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n",
    "        if repetition_penalty != 1.0:\n",
    "            for i in range(batch_size * num_beams):\n",
    "                for previous_token in set(tgt[i].tolist()):\n",
    "                    # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n",
    "                    if scores[i, previous_token] < 0:\n",
    "                        scores[i, previous_token] *= repetition_penalty\n",
    "                    else:\n",
    "                        scores[i, previous_token] /= repetition_penalty\n",
    "\n",
    "        if do_sample:\n",
    "            # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "            if temperature != 1.0:\n",
    "                scores = scores / temperature\n",
    "            # Top-p/top-k filtering\n",
    "            scores = top_k_top_p_filtering(\n",
    "                scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=2\n",
    "            )  # (batch_size * num_beams, vocab_size)\n",
    "            # Sample 2 next words for each beam (so we have some spare tokens and match output of greedy beam search)\n",
    "            next_words = torch.multinomial(F.softmax(scores, dim=-1), num_samples=2)  # (batch_size * num_beams, 2)\n",
    "            # Compute next scores\n",
    "            _scores = F.log_softmax(scores, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "            _scores = torch.gather(_scores, -1, next_words)  # (batch_size * num_beams, 2)\n",
    "            next_scores = _scores + beam_scores[:, None].expand_as(_scores)  # (batch_size * num_beams, 2)\n",
    "            # Match shape of greedy beam search\n",
    "            next_words = next_words.view(batch_size, 2 * num_beams)  # (batch_size, 2 * num_beams)\n",
    "            next_scores = next_scores.view(batch_size, 2 * num_beams)  # (batch_size, 2 * num_beams)\n",
    "        else:\n",
    "            # do greedy beam search\n",
    "            scores = F.log_softmax(scores, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "            assert scores.size() == (batch_size * num_beams, vocab_size)\n",
    "            # Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product)\n",
    "            _scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n",
    "            # re-organize to group the beam together (we are keeping top hypothesis accross beams)\n",
    "            _scores = _scores.view(batch_size, num_beams * vocab_size)  # (batch_size, num_beams * vocab_size)\n",
    "            next_scores, next_words = torch.topk(_scores, 2 * num_beams, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        assert next_scores.size() == next_words.size() == (batch_size, 2 * num_beams)\n",
    "\n",
    "        # next batch beam content\n",
    "        # list of (batch_size * num_beams) tuple(next hypothesis score, next word, current position in the batch)\n",
    "        next_batch_beam = []\n",
    "\n",
    "        # for each sentence\n",
    "        for batch_ex in range(batch_size):\n",
    "\n",
    "            # if we are done with this sentence\n",
    "            done[batch_ex] = done[batch_ex] or generated_hyps[batch_ex].is_done(next_scores[batch_ex].max().item())\n",
    "            if done[batch_ex]:\n",
    "                next_batch_beam.extend([(0, pad_token_id, 0)] * num_beams)  # pad the batch\n",
    "                continue\n",
    "\n",
    "            # next sentence beam content\n",
    "            next_sent_beam = []\n",
    "\n",
    "            # next words for this sentence\n",
    "            for idx, score in zip(next_words[batch_ex], next_scores[batch_ex]):\n",
    "\n",
    "                # get beam and word IDs\n",
    "                beam_id = idx // vocab_size\n",
    "                word_id = idx % vocab_size\n",
    "\n",
    "                # end of sentence, or next word\n",
    "                if word_id.item() in eos_token_ids or cur_len + 1 == max_length:\n",
    "                    generated_hyps[batch_ex].add(\n",
    "                        tgt[batch_ex * num_beams + beam_id, :cur_len].clone(), score.item()\n",
    "                    )\n",
    "                else:\n",
    "                    next_sent_beam.append((score, word_id, batch_ex * num_beams + beam_id))\n",
    "\n",
    "                # the beam for next step is full\n",
    "                if len(next_sent_beam) == num_beams:\n",
    "                    break\n",
    "\n",
    "            # update next beam content\n",
    "            assert len(next_sent_beam) == 0 if cur_len + 1 == max_length else num_beams\n",
    "            if len(next_sent_beam) == 0:\n",
    "                next_sent_beam = [(0, pad_token_id, 0)] * num_beams  # pad the batch\n",
    "            next_batch_beam.extend(next_sent_beam)\n",
    "            assert len(next_batch_beam) == num_beams * (batch_ex + 1)\n",
    "\n",
    "        # sanity check / prepare next batch\n",
    "        assert len(next_batch_beam) == batch_size * num_beams\n",
    "        beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
    "        beam_words = tgt.new([x[1] for x in next_batch_beam])\n",
    "        beam_idx = tgt.new([x[2] for x in next_batch_beam])\n",
    "\n",
    "        # re-order batch\n",
    "        tgt = tgt[beam_idx, :]\n",
    "        tgt = torch.cat([tgt, beam_words.unsqueeze(1)], dim=-1)\n",
    "\n",
    "        # re-order internal states\n",
    "        if past is not None:\n",
    "            reordered_past = []\n",
    "            for layer_past in past:\n",
    "                # get the correct batch idx from layer past batch dim\n",
    "                # batch dim of `past` and `mems` is at 2nd position\n",
    "                reordered_layer_past = [layer_past[:, i].unsqueeze(1).clone().detach() for i in beam_idx]\n",
    "                reordered_layer_past = torch.cat(reordered_layer_past, dim=1)\n",
    "                # check that shape matches\n",
    "                assert reordered_layer_past.shape == layer_past.shape\n",
    "                reordered_past.append(reordered_layer_past)\n",
    "            past = tuple(reordered_past)\n",
    "\n",
    "        # update current length\n",
    "        cur_len = cur_len + 1\n",
    "\n",
    "        # stop when we are done with each sentence\n",
    "        if all(done):\n",
    "            break\n",
    "\n",
    "    # visualize hypotheses\n",
    "    # print([len(x) for x in generated_hyps], cur_len)\n",
    "    # globals().update( locals() );\n",
    "    # !import code; code.interact(local=vars())\n",
    "    # for ii in range(batch_size):\n",
    "    #     for ss, ww in sorted(generated_hyps[ii].hyp, key=lambda x: x[0], reverse=True):\n",
    "    #         print(\"%.3f \" % ss + \" \".join(self.dico[x] for x in ww.tolist()))\n",
    "    #     print(\"\")\n",
    "\n",
    "    # select the best hypotheses\n",
    "    tgt_len = tgt.new(batch_size)\n",
    "    best = []\n",
    "\n",
    "    for i, hypotheses in enumerate(generated_hyps):\n",
    "        best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
    "        tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
    "        best.append(best_hyp)\n",
    "\n",
    "    # generate target batch\n",
    "    decoded = tgt.new(batch_size, tgt_len.max().item()).fill_(pad_token_id)\n",
    "    for i, hypo in enumerate(best):\n",
    "        decoded[i, : tgt_len[i] - 1] = hypo\n",
    "        decoded[i, tgt_len[i] - 1] = eos_token_ids[0]\n",
    "\n",
    "    return decoded # (b, (tgt_seq_len <= ? <= max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=20\n",
    "generate_args = dict(   \n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_k=3,\n",
    "    top_p=0.5,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=pad_token_id,\n",
    "    eos_token_ids=[eos_token_id],\n",
    "    length_penalty=1,\n",
    "    num_beams=4,\n",
    "    vocab_size=tgt_vocab_size,\n",
    ")\n",
    "\n",
    "# With memory, Without past\n",
    "generated_decoder = GeneratedLM(decoder, tgt_vocab_size, support_past=False)\n",
    "tgt = torch.zeros((bs, 1), dtype=torch.long).fill_(bos_token_id)\n",
    "result = generated_decoder._generate_beam_search(tgt, **generate_args, model_otherargs=[memory])\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= 1\n",
    "\n",
    "# Without memory, With past\n",
    "generated_lm = GeneratedLM(lm, tgt_vocab_size, support_past=True)\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "result = generated_lm._generate_beam_search(tgt, **generate_args)\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= tgt_seq_len\n",
    "\n",
    "# Without memory, Without past\n",
    "generated_lm = GeneratedLM(lm, tgt_vocab_size, support_past=False)\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "result = generated_lm._generate_beam_search(tgt, **generate_args)\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= tgt_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    self:GeneratedLM,\n",
    "    tgt,\n",
    "    max_length=20,\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1,\n",
    "    pad_token_id=None,\n",
    "    eos_token_ids=None,\n",
    "    length_penalty=1.0,\n",
    "    model_otherargs=[],\n",
    "    model_otherkwargs={},\n",
    "):\n",
    "    '''\n",
    "    tgt: (b, tgt_seq_len)\n",
    "    model_otherargs: Other positional args that your model need. Maybe momory from encoder.\n",
    "    model_otherkwargs: Other keyword args that your model need. Maybe some masks.\n",
    "    returns: (b, (tgt_seq_len <= ? <= max_length))\n",
    "    '''\n",
    "    assert isinstance(max_length, int) and max_length > 0, \"`max_length` should be a strictely positive integer.\"\n",
    "    assert isinstance(do_sample, bool), \"`do_sample` should be a boolean.\"\n",
    "    assert isinstance(num_beams, int) and num_beams > 0, \"`num_beams` should be a strictely positive integer.\"\n",
    "    assert temperature > 0, \"`temperature` should be strictely positive.\"\n",
    "    assert isinstance(top_k, int) and top_k >= 0, \"`top_k` should be a positive integer.\"\n",
    "    assert 0 <= top_p <= 1, \"`top_p` should be between 0 and 1.\"\n",
    "    assert repetition_penalty >= 1.0, \"`repetition_penalty` should be >= 1.\"\n",
    "    assert isinstance(pad_token_id, int) and pad_token_id >= 0, \"`pad_token_id` should be a positive integer.\"\n",
    "    assert isinstance(eos_token_ids, (list, tuple)) and (\n",
    "        e >= 0 for e in eos_token_ids\n",
    "    ), \"`eos_token_ids` should be a positive integer or a list/tuple of positive integers.\"\n",
    "    assert length_penalty > 0, \"`length_penalty` should be strictely positive.\"\n",
    "\n",
    "\n",
    "\n",
    "    if num_beams > 1:\n",
    "        output = self._generate_beam_search(\n",
    "            tgt,\n",
    "            max_length,\n",
    "            do_sample,\n",
    "            temperature,\n",
    "            top_k,\n",
    "            top_p,\n",
    "            repetition_penalty,\n",
    "            pad_token_id,\n",
    "            eos_token_ids,\n",
    "            length_penalty,\n",
    "            num_beams,\n",
    "            self.vocab_size,\n",
    "            model_otherargs,\n",
    "            model_otherkwargs,\n",
    "        )\n",
    "    else:\n",
    "        output = self._generate_no_beam_search(\n",
    "            tgt,\n",
    "            max_length,\n",
    "            do_sample,\n",
    "            temperature,\n",
    "            top_k,\n",
    "            top_p,\n",
    "            repetition_penalty,\n",
    "            pad_token_id,\n",
    "            eos_token_ids,\n",
    "            model_otherargs,\n",
    "            model_otherkwargs,\n",
    "        )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=20\n",
    "generate_args = dict(   \n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    temperature=0.1,\n",
    "    top_k=3,\n",
    "    top_p=0.5,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=pad_token_id,\n",
    "    eos_token_ids=[eos_token_id],\n",
    "    length_penalty=1,\n",
    ")\n",
    "\n",
    "generated_decoder = GeneratedLM(decoder, tgt_vocab_size, support_past=False)\n",
    "tgt = torch.zeros((bs, 1), dtype=torch.long).fill_(bos_token_id)\n",
    "result = generated_decoder.generate(tgt, **generate_args, model_otherargs=[memory])\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= 1\n",
    "\n",
    "generated_lm = GeneratedLM(lm, tgt_vocab_size, support_past=True)\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "result = generated_lm.generate(tgt, **generate_args)\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= tgt_seq_len\n",
    "\n",
    "generated_lm = GeneratedLM(lm, tgt_vocab_size, support_past=False)\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "result = generated_lm.generate(tgt, **generate_args)\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= tgt_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=20\n",
    "generate_args = dict(   \n",
    "    max_length=max_length,\n",
    "    do_sample=True,\n",
    "    num_beams=2,\n",
    "    temperature=0.1,\n",
    "    top_k=3,\n",
    "    top_p=0.5,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=pad_token_id,\n",
    "    eos_token_ids=[eos_token_id],\n",
    "    length_penalty=1,\n",
    ")\n",
    "\n",
    "# With memory, Without past\n",
    "generated_decoder = GeneratedLM(decoder, tgt_vocab_size, support_past=False)\n",
    "tgt = torch.zeros((bs, 1), dtype=torch.long).fill_(bos_token_id)\n",
    "result = generated_decoder.generate(tgt, **generate_args, model_otherargs=[memory])\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= 1\n",
    "\n",
    "# Without memory, With past\n",
    "generated_lm = GeneratedLM(lm, tgt_vocab_size, support_past=True)\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "result = generated_lm.generate(tgt, **generate_args)\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= tgt_seq_len\n",
    "\n",
    "# Without memory, Without past\n",
    "generated_lm = GeneratedLM(lm, tgt_vocab_size, support_past=False)\n",
    "tgt = torch.randint(0, tgt_vocab_size-2, (bs, tgt_seq_len))\n",
    "result = generated_lm.generate(tgt, **generate_args)\n",
    "test_eq(result.shape[0], bs)\n",
    "assert result.shape[1] <= max_length and result.shape[1] >= tgt_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that with do_sample=False, GeneratedLM.generate should returns the same result as huggingface's PretrainedModel.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "lm = AutoModelWithLMHead.from_pretrained('distilgpt2')\n",
    "lm.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "sentence = 'The dog is a'\n",
    "tgt = torch.tensor([tokenizer.encode(sentence)])\n",
    "\n",
    "generate_args = dict(   \n",
    "    max_length=20,\n",
    "    do_sample=False,\n",
    "    num_beams=1,\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=1,\n",
    "    pad_token_id=0,\n",
    "    eos_token_ids=[tokenizer.eos_token_id],\n",
    "    length_penalty=1.0,\n",
    ")\n",
    "generated_lm = GeneratedLM(lm, tokenizer.vocab_size, True)\n",
    "numeric_result = generated_lm.generate(tgt, **generate_args)\n",
    "result = tokenizer.decode(numeric_result[0])\n",
    "\n",
    "huggingface_numeric_result = lm.generate(tgt, **generate_args)\n",
    "huggingface_result = tokenizer.decode(huggingface_numeric_result[0])\n",
    "\n",
    "test_eq(result, huggingface_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_general.ipynb.\n",
      "Converted 01a_transforms.ipynb.\n",
      "Converted 01b_model_splits.ipynb.\n",
      "Converted 01c_callbacks.ipynb.\n",
      "Converted 02_generated_lm.ipynb.\n",
      "Converted 99a_example_roberta_classification.ipynb.\n",
      "Converted 99b_example_gpt2_lm.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
