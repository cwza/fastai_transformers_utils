{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from fastai2.text.all import *\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "from fastai_transformers_utils.generated_lm import GeneratedLM, GenerateArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: GeneratedLM\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used on Hunggingface's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model and vocab\n",
    "lm = AutoModelWithLMHead.from_pretrained('distilgpt2')\n",
    "lm.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dog\n",
      "The dog can never understand a woman. So in the early weeks, it made!\n",
      "The dog is at least ten years pregnant\n",
      "\n",
      "\n",
      "\n",
      "He'll be back at school with!\n",
      "The dog, in its early 90s, has been a fixture on the scene since. For!\n"
     ]
    }
   ],
   "source": [
    "num_returns = 3\n",
    "sentence = 'The dog'\n",
    "tgt = torch.tensor([tokenizer.encode(sentence)] * num_returns)\n",
    "\n",
    "generate_args = GenerateArgs(   \n",
    "    max_length=20,\n",
    "    do_sample=True,\n",
    "    num_beams=5,\n",
    "    temperature=1.5,\n",
    "    top_k=50,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1,\n",
    "    length_penalty=1.0,\n",
    ")\n",
    "generated_lm = GeneratedLM(lm, tokenizer.vocab_size, lm.config.pad_token_id, [lm.config.eos_token_ids], True)\n",
    "numeric_result = generated_lm.generate(tgt, generate_args)\n",
    "\n",
    "for i in range(num_returns):\n",
    "    result = tokenizer.decode(numeric_result[i], skip_special_tokens=True)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used on Fastai2 AWD_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model and vocab\n",
    "path = untar_data(URLs.WT103_FWD)\n",
    "vocab = list(path.glob('*.pkl'))[0].load()\n",
    "model_weights = torch.load(list(path.glob('*.pth'))[0], map_location = lambda storage,loc: storage)\n",
    "model = get_language_model(AWD_LSTM, len(vocab))\n",
    "load_ignore_keys(model, model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    5,    9, 2235],\n",
       "        [   2,    5,    9, 2235]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and Numericalize\n",
    "tokenizer = Tokenizer(SpacyTokenizer())\n",
    "numericalizer = Numericalize(vocab=vocab)\n",
    "pipe = Pipeline([tokenizer, numericalizer], True)\n",
    "\n",
    "num_returns = 2\n",
    "sentence = 'The dog'\n",
    "tgt = torch.stack([pipe(sentence)] * num_returns, dim=0)\n",
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxbos xxmaj the dog show xxmaj animal of xxmaj flesh while on its writing page from the back of xxeos\n",
      "xxbos xxmaj the dog race xxup sm - xxunk = \n",
      "  xxmaj xxunk was a nickname xxmaj xxunk â€“ xxeos\n"
     ]
    }
   ],
   "source": [
    "# Generate and Decode\n",
    "generate_args = GenerateArgs(   \n",
    "    max_length=20,\n",
    "    do_sample=True,\n",
    "    num_beams=5,\n",
    "    temperature=1.5,\n",
    "    top_k=50,\n",
    "    top_p=1,\n",
    "    repetition_penalty=1,\n",
    "    length_penalty=1.0,\n",
    ")\n",
    "generated_lm = GeneratedLM(model, len(vocab), awd_lstm_lm_config['pad_token'], [3], False)\n",
    "numeric_result = generated_lm.generate(tgt, generate_args)\n",
    "\n",
    "for i in range(num_returns):\n",
    "    result = pipe.decode(numeric_result[i])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
