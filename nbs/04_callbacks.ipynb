{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from fastai2.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "> Handle the the different format of inputs and outputs between fastai and transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FakeLearner Class just for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeLearner():\n",
    "    def __init__(self, cb, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        cb.learn = self\n",
    "        self.cb = cb\n",
    "    \n",
    "    def run_cb(self, event_name):\n",
    "        getattr(self.cb, event_name)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2LMHeadCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GPT2LMHeadCallback(Callback):\n",
    "    def after_pred(self):\n",
    "        ''' The output of AutoModelWithLMHead is (last_hidden_state, past)\n",
    "            What fastai want is last_hidden_state '''\n",
    "        last_hidden_state = self.learn.pred[0]\n",
    "        self.learn.pred = last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = FakeLearner(cb=GPT2LMHeadCallback(), pred=('last_hidden_state', 'past'))\n",
    "learn.run_cb('after_pred')\n",
    "test_eq(learn.pred, 'last_hidden_state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertSeqClassificationCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BertSeqClassificationCallback(Callback):\n",
    "    ''' It should be ok to use it in all Bert like model. eg: Roberta\n",
    "    '''\n",
    "    def __init__(self, pad_id: int):\n",
    "        self.pad_id = pad_id\n",
    "    \n",
    "    def begin_batch(self):\n",
    "        ''' Instead of input_ids, we need to pass the attention_mask to AutoModelForSequenceClassification to avoid it to attention to padding tokens.\n",
    "        '''\n",
    "        input_ids = self.learn.xb[0]\n",
    "        device = input_ids.device\n",
    "        attention_mask = torch.where(input_ids == self.pad_id, torch.tensor(0, device=device), torch.tensor(1, device=device)).to(input_ids)\n",
    "        self.learn.xb = [input_ids, attention_mask]\n",
    "    \n",
    "    def after_pred(self):\n",
    "        ''' The output of AutoModelForSequenceClassification is (logits, )\n",
    "            What fastai want is logits '''\n",
    "        logits = self.learn.pred[0]\n",
    "        self.learn.pred = logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[4, 3, 1, 1], \n",
    "                          [5, 6, 7, 1]])\n",
    "attention_mask = torch.tensor([[1, 1, 0, 0], \n",
    "                               [1, 1, 1, 0]])\n",
    "\n",
    "learn = FakeLearner(cb=BertSeqClassificationCallback(pad_id=1), xb=(input_ids,))\n",
    "learn.run_cb('begin_batch')\n",
    "test_eq(learn.xb, (input_ids, attention_mask))\n",
    "\n",
    "learn = FakeLearner(cb=BertSeqClassificationCallback(pad_id=1), pred=('logits',))\n",
    "learn.run_cb('after_pred')\n",
    "test_eq(learn.pred, 'logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
