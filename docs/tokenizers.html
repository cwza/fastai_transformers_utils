---

title: Tokenizers

keywords: fastai
sidebar: home_sidebar


---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01_tokenizers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<p style="color: red;">
The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>
We recommend you <a href="https://www.tensorflow.org/guide/migrate" target="_blank">upgrade</a> now 
or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:
<a href="https://colab.research.google.com/notebooks/tensorflow_version.ipynb" target="_blank">more info</a>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="TransformsTokenizer">TransformsTokenizer<a class="anchor-link" href="#TransformsTokenizer">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TransformersTokenizer" class="doc_header"><code>class</code> <code>TransformersTokenizer</code><a href="https://github.com/cwza/fastai_transformers_utils/tree/master/fastai_transformers_utils/tokenizers.py#L13" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TransformersTokenizer</code>(<strong><code>tokenizer</code></strong>:<code>PreTrainedTokenizer</code>)</p>
</blockquote>
<p>fastai want the tokenizer can handle list of string.
use in parallel_gen()</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;This is a test&#39;</span><span class="p">,</span> <span class="s1">&#39;Just test&#39;</span><span class="p">]</span>
<span class="n">transfomersTokenizer</span> <span class="o">=</span> <span class="n">TransformersTokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">tok_texts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">transfomersTokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">))</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">tok_texts</span><span class="p">,</span> <span class="p">[[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;just&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">]])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;This is a test&#39;</span><span class="p">,</span> <span class="s1">&#39;Just test&#39;</span><span class="p">]</span>
<span class="c1"># parallel_gen will return generator of (0, [&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;test&#39;]), (1, [&#39;just&#39;, &#39;test&#39;])</span>
<span class="n">tok_texts</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">parallel_gen</span><span class="p">(</span><span class="n">TransformersTokenizer</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">))</span><span class="o">.</span><span class="n">sorted</span><span class="p">()</span><span class="o">.</span><span class="n">itemgot</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">tok_texts</span><span class="p">,</span> <span class="p">[[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],[</span><span class="s1">&#39;just&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">]])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="GPT2DecoderTokenizer">GPT2DecoderTokenizer<a class="anchor-link" href="#GPT2DecoderTokenizer">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GPT2DecoderTokenizer" class="doc_header"><code>class</code> <code>GPT2DecoderTokenizer</code><a href="https://github.com/cwza/fastai_transformers_utils/tree/master/fastai_transformers_utils/tokenizers.py#L22" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GPT2DecoderTokenizer</code>(<strong>*<code>inputs</code></strong>, <strong>**<code>kwargs</code></strong>) :: <code>GPT2Tokenizer</code></p>
</blockquote>
<p>Add special tokens: &lt;|bos|&gt;, &lt;|pad|&gt;.
Add &lt;|bos|&gt; to the begin of the tokenized string and add &lt;|endoftext|&gt; to the end of the tokenized string.
For the decoder of machine translation</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2DecoderTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilgpt2&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s1">&#39;The dog.&#39;</span>
<span class="n">test_eq</span><span class="p">(</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="p">[</span><span class="s1">&#39;&lt;|bos|&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;The&#39;</span><span class="p">,</span> <span class="s1">&#39;Ä dog&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;|endoftext|&gt;&#39;</span><span class="p">]</span> <span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="p">[</span><span class="mi">50257</span><span class="p">,</span> <span class="mi">464</span><span class="p">,</span> <span class="mi">3290</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">50256</span><span class="p">]</span> <span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">pad_to_max_length</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="p">[</span><span class="mi">50257</span><span class="p">,</span> <span class="mi">464</span><span class="p">,</span> <span class="mi">3290</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">50256</span><span class="p">,</span> <span class="mi">50258</span><span class="p">]</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>
 

