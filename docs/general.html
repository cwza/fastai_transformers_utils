---

title: General

keywords: fastai
sidebar: home_sidebar


---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/00_general.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<p style="color: red;">
The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>
We recommend you <a href="https://www.tensorflow.org/guide/migrate" target="_blank">upgrade</a> now 
or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:
<a href="https://colab.research.google.com/notebooks/tensorflow_version.ipynb" target="_blank">more info</a>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TransformersTokenizer" class="doc_header"><code>class</code> <code>TransformersTokenizer</code><a href="https://github.com/cwza/fastai_transformers_utils/tree/master/fastai_transformers_utils/general.py#L14" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TransformersTokenizer</code>(<strong><code>tokenizer</code></strong>:<code>PreTrainedTokenizer</code>)</p>
</blockquote>
<p>fastai want the tokenizer can handle list of string.
use in parallel_gen()</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;This is a test&#39;</span><span class="p">,</span> <span class="s1">&#39;Just test&#39;</span><span class="p">]</span>
<span class="n">transfomersTokenizer</span> <span class="o">=</span> <span class="n">TransformersTokenizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">tok_texts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">transfomersTokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">))</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">tok_texts</span><span class="p">,</span> <span class="p">[[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;just&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">]])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;This is a test&#39;</span><span class="p">,</span> <span class="s1">&#39;Just test&#39;</span><span class="p">]</span>
<span class="c1"># parallel_gen will return generator of (0, [&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;test&#39;]), (1, [&#39;just&#39;, &#39;test&#39;])</span>
<span class="n">tok_texts</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">parallel_gen</span><span class="p">(</span><span class="n">TransformersTokenizer</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">))</span><span class="o">.</span><span class="n">sorted</span><span class="p">()</span><span class="o">.</span><span class="n">itemgot</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">test_eq</span><span class="p">(</span><span class="n">tok_texts</span><span class="p">,</span> <span class="p">[[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],[</span><span class="s1">&#39;just&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">]])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
</div>
 

