---

title: Example: Roberta Classification on IMDB_SAMPLE

keywords: fastai
sidebar: home_sidebar

summary: "This is also an integration test."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/99a_example_roberta_classification.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">capture</span>
from fastai_transformers_utils.all import TransformersTokenizer, TransformersNumericalize, Pad2Max, BertSeqClassificationCallback, roberta_SeqClassification_split

import json
from typing import *
from fastai2.basics import *
from fastai2.text.all import *
from fastai2.callback.all import *

from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification, RobertaTokenizer
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># all_slow</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_class</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># model_name = &#39;bert-base-uncased&#39;</span>
<span class="c1"># model_name = &#39;distilbert-base-uncased&#39;</span>
<span class="c1"># model_name = &#39;albert-base-v2&#39;</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;roberta-base&#39;</span>
<span class="n">max_len</span> <span class="o">=</span> <span class="mi">150</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Data-and-Tokenization">Data and Tokenization<a class="anchor-link" href="#Data-and-Tokenization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMDB_SAMPLE</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;texts.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>negative</td>
      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>positive</td>
      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>negative</td>
      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>positive</td>
      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie "Duty, Honor, Country" are not just mere words blathered from the lips of a high-brassed offic...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>negative</td>
      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_list</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">parallel_gen</span><span class="p">(</span><span class="n">TransformersTokenizer</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">sorted</span><span class="p">()</span><span class="o">.</span><span class="n">itemgot</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tok_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">tok_df</span><span class="o">.</span><span class="n">text</span> <span class="o">=</span>  <span class="n">tok_list</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># split tokens by &#39; &#39;</span>
<span class="n">tok_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>negative</td>
      <td>Un - ble eping - bel iev able ! ĠMeg ĠRyan Ġdoesn 't Ġeven Ġlook Ġher Ġusual Ġpert Ġl ovable Ġself Ġin Ġthis , Ġwhich Ġnormally Ġmakes Ġme Ġforgive Ġher Ġshallow Ġtick y Ġacting Ġsch tick . ĠHard Ġto Ġbelieve Ġshe Ġwas Ġthe Ġproducer Ġon Ġthis Ġdog . ĠPlus ĠKevin ĠK line : Ġwhat Ġkind Ġof Ġsuicide Ġtrip Ġhas Ġhis Ġcareer Ġbeen Ġon ? ĠWho osh ... ĠBan zai !!! ĠFinally Ġthis Ġwas Ġdirected Ġby Ġthe Ġguy Ġwho Ġdid ĠBig ĠChill ? ĠMust Ġbe Ġa Ġreplay Ġof ĠJon est own Ġ- Ġh ollywood Ġstyle . ĠWoo off f !</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>positive</td>
      <td>This Ġis Ġa Ġextremely Ġwell - made Ġfilm . ĠThe Ġacting , Ġscript Ġand Ġcamera - work Ġare Ġall Ġfirst - rate . ĠThe Ġmusic Ġis Ġgood , Ġtoo , Ġthough Ġit Ġis Ġmostly Ġearly Ġin Ġthe Ġfilm , Ġwhen Ġthings Ġare Ġstill Ġrelatively Ġche ery . ĠThere Ġare Ġno Ġreally Ġsuperst ars Ġin Ġthe Ġcast , Ġthough Ġseveral Ġfaces Ġwill Ġbe Ġfamiliar . ĠThe Ġentire Ġcast Ġdoes Ġan Ġexcellent Ġjob Ġwith Ġthe Ġscript .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; But Ġit Ġis Ġhard Ġto Ġwatch , Ġbecause Ġthere Ġis Ġno Ġgood Ġend Ġto Ġa Ġsituation Ġlike Ġthe Ġone Ġpresented . ĠIt Ġis Ġnow Ġfashionable Ġto Ġblame Ġthe ĠBritish Ġfor Ġse...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>negative</td>
      <td>Every Ġonce Ġin Ġa Ġlong Ġwhile Ġa Ġmovie Ġwill Ġcome Ġalong Ġthat Ġwill Ġbe Ġso Ġawful Ġthat ĠI Ġfeel Ġcompelled Ġto Ġwarn Ġpeople . ĠIf ĠI Ġlabor Ġall Ġmy Ġdays Ġand ĠI Ġcan Ġsave Ġbut Ġone Ġsoul Ġfrom Ġwatching Ġthis Ġmovie , Ġhow Ġgreat Ġwill Ġbe Ġmy Ġjoy .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; Where Ġto Ġbegin Ġmy Ġdiscussion Ġof Ġpain . ĠFor Ġstarters , Ġthere Ġwas Ġa Ġmusical Ġmont age Ġevery Ġfive Ġminutes . ĠThere Ġwas Ġno Ġcharacter Ġdevelopment . ĠEvery Ġcharacter Ġwas Ġa Ġstereotype . ĠWe Ġhad Ġswearing Ġguy , Ġfat Ġguy Ġwho Ġeats Ġdon uts , Ġgoofy Ġforeign Ġguy , Ġetc . ĠThe Ġscript Ġfelt Ġas Ġif ...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>positive</td>
      <td>Name Ġjust Ġsays Ġit Ġall . ĠI Ġwatched Ġthis Ġmovie Ġwith Ġmy Ġdad Ġwhen Ġit Ġcame Ġout Ġand Ġhaving Ġserved Ġin ĠKorea Ġhe Ġhad Ġgreat Ġadmiration Ġfor Ġthe Ġman . ĠThe Ġdisappointing Ġthing Ġabout Ġthis Ġfilm Ġis Ġthat Ġit Ġonly Ġconcentrate Ġon Ġa Ġshort Ġperiod Ġof Ġthe Ġman 's Ġlife Ġ- Ġinterestingly Ġenough Ġthe Ġman 's Ġentire Ġlife Ġwould Ġhave Ġmade Ġsuch Ġan Ġepic Ġbio - pic Ġthat Ġit Ġis Ġstaggering Ġto Ġimagine Ġthe Ġcost Ġfor Ġproduction .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; Some Ġposters Ġel ude Ġto Ġthe Ġflawed Ġcharacteristics Ġabout Ġthe Ġman , Ġwhich Ġare Ġcheap Ġshots . ĠThe Ġtheme Ġof Ġt...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>negative</td>
      <td>This Ġmovie Ġsucceeds Ġat Ġbeing Ġone Ġof Ġthe Ġmost Ġunique Ġmovies Ġyou 've Ġseen . ĠHowever Ġthis Ġcomes Ġfrom Ġthe Ġfact Ġthat Ġyou Ġcan 't Ġmake Ġheads Ġor Ġtails Ġof Ġthis Ġmess . ĠIt Ġalmost Ġseems Ġas Ġa Ġseries Ġof Ġchallenges Ġset Ġup Ġto Ġdetermine Ġwhether Ġor Ġnot Ġyou Ġare Ġwilling Ġto Ġwalk Ġout Ġof Ġthe Ġmovie Ġand Ġgive Ġup Ġthe Ġmoney Ġyou Ġjust Ġpaid . ĠIf Ġyou Ġdon 't Ġwant Ġto Ġfeel Ġslight ed Ġyou 'll Ġsit Ġthrough Ġthis Ġhorrible Ġfilm Ġand Ġdevelop Ġa Ġreal Ġsense Ġof Ġpity Ġfor Ġthe Ġactors Ġinvolved , Ġthey 've Ġall Ġseen Ġbetter Ġdays , Ġbut Ġthen Ġyou Ġrealize Ġ...</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tok_df.to_csv(f&#39;{model_name}_tok.csv&#39;, index=False)</span>
<span class="c1"># tok_df = pd.read_csv(f&#39;{model_name}_tok.csv&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Databunch">Databunch<a class="anchor-link" href="#Databunch">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">splits</span> <span class="o">=</span> <span class="n">ColSplitter</span><span class="p">()(</span><span class="n">tok_df</span><span class="p">)</span>
<span class="n">ds_tfms</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">attrgetter</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">),</span> <span class="n">TransformersNumericalize</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),</span> <span class="n">Pad2Max</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)],</span> 
    <span class="p">[</span><span class="n">attrgetter</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">),</span> <span class="n">Categorize</span><span class="p">()]</span>
<span class="p">]</span>
<span class="n">dsrc</span> <span class="o">=</span> <span class="n">DataSource</span><span class="p">(</span><span class="n">tok_df</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">ds_tfms</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">)</span>
<span class="n">dsrc</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dsrc</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">dsrc</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((tensor([    0,  9685,    12,  5225, 24320,    12,  8494, 18421,   868,   328,
          14938,  1774,   630,    75,   190,   356,    69,  4505, 32819,   784,
          30289,  1403,    11,    42,     6,    61,  6329,   817,   162, 20184,
             69, 16762, 10457,   219,  3501,  8447, 41791,     4,  6206,     7,
            679,    79,    21,     5,  3436,    15,    42,  2335,     4,  4642,
           2363,   229,  1902,    35,    99,   761,     9,  4260,  1805,    34,
             39,   756,    57,    15,   116,  3394,  5212,   734,  5981, 23642,
          16506,  3347,    42,    21,  3660,    30,     5,  2173,    54,   222,
           1776, 25928,   116,  8495,    28,    10, 16462,     9,  4160,   990,
           3355,   111,  1368,  9718,  2496,     4, 29935,  1529,   506,   328,
              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),
  tensor(0)),
 (&#34;&lt;s&gt;Un-bleeping-believable! Meg Ryan doesn&#39;t even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!&lt;/s&gt;&#34;,
  &#39;negative&#39;))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_databunch</span><span class="p">(</span><span class="n">dsrc</span><span class="p">,</span> <span class="n">bs</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dsrc</span><span class="o">.</span><span class="n">databunch</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">after_batch</span><span class="o">=</span><span class="p">[</span><span class="n">Cuda</span><span class="p">()])</span>

<span class="n">dbunch</span> <span class="o">=</span> <span class="n">get_databunch</span><span class="p">(</span><span class="n">dsrc</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># for x in dbunch.train_dl:</span>
<span class="c1">#     print(x[0].shape, x[0].dtype)</span>
<span class="c1">#     print(x[1].shape, x[1].dtype)</span>
<span class="c1">#     break</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dbunch</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>&lt;s&gt;I'm glad that I saw this film after Mr.Sandler became famous.&lt;br /&gt;&lt;br /&gt;It is bad....bad,bad,bad. There is no plot. It's like watching a painfully dull home movie.&lt;br /&gt;&lt;br /&gt;I really enjoy his other films......but if you're a fan like me....stay away from this one. It may change your thoughts on Adam. You may never recover from the horror that is this film....I've had a better time watching old folks play scrabble in a home.......&lt;/s&gt;</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>&lt;s&gt;As a former 2 time Okinawan Karate world champion, I like movies about sacrifice for sport. But this movie is about so much more. This movie is so good and so deep. I have recently been plagued by very serious injury and pretty much a disastrous lack of passion. Almost lights out for me. And this silly little movie touched me so deep that like out of a daze it reminded me about what life is supposed to be about. This is a movie about living. Living your life for yourself and respect for others. Empowerment. God, bless "Bend it like Beckham" I believe it is a true gem.&lt;/s&gt;</td>
      <td>positive</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learner-and-Train">Learner and Train<a class="anchor-link" href="#Learner-and-Train">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dbunch</span> <span class="o">=</span> <span class="n">get_databunch</span><span class="p">(</span><span class="n">dsrc</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="o">=</span><span class="n">num_class</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dbunch</span><span class="p">,</span> 
                <span class="n">model</span><span class="p">,</span> 
                <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span> 
                <span class="n">opt_func</span><span class="o">=</span><span class="n">ranger</span><span class="p">,</span>
                <span class="n">splitter</span><span class="o">=</span><span class="n">roberta_SeqClassification_split</span><span class="p">,</span> 
                <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">BertSeqClassificationCallback</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)],</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">],</span>
               <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>RobertaForSequenceClassification (Input shape: 64 x 150)
================================================================
Layer (type)         Output Shape         Param #    Trainable 
================================================================
Embedding            64 x 150 x 768       38,603,520 False     
________________________________________________________________
Embedding            64 x 150 x 768       394,752    False     
________________________________________________________________
Embedding            64 x 150 x 768       768        False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 768             590,592    False     
________________________________________________________________
Tanh                 64 x 768             0          False     
________________________________________________________________
Linear               64 x 768             590,592    True      
________________________________________________________________
Dropout              64 x 768             0          False     
________________________________________________________________
Linear               64 x 2               1,538      True      
________________________________________________________________

Total params: 125,237,762
Total trainable params: 592,130
Total non-trainable params: 124,645,632

Optimizer used: &lt;function ranger at 0x7fcdb00ab950&gt;
Loss function: FlattenedLoss of CrossEntropyLoss()

Model frozen up to parameter group number 14

Callbacks:
  - TrainEvalCallback
  - Recorder
  - ProgressCallback
  - BertSeqClassificationCallback</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.lr_find()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">0.8</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.697571</td>
      <td>0.688053</td>
      <td>0.555000</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.fit_one_cycle(5, 1e-2, moms=(0.8,0.7,0.8))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.freeze_to(-3)</span>
<span class="c1"># learn.fit_one_cycle(5, 1e-3, moms=(0.8,0.7,0.8))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">show_results</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
      <th>category_</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>&lt;s&gt;I love this movie. My friend Marcus and I were browsing the local Hastings because we had an urge to rent something we had never seen before and stumbled across this fine film. We had no idea what it was going to be about, but it turned out spectacular. 2 thumbs up. I liked how the film was shot, and the actors were very funny. If you are are looking for a funny movie that also makes you think I highly suggest you quickly run to your local video store and find this movie. I would tell you some of my favorite parts but that might ruin the film for you so I won't. This movie is definitely on my top 10 list of good movies. Do you really think Nothing is bouncy?&lt;/s&gt;</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>&lt;s&gt;To be honest i had heard this was pretty bad before i decided to watch it, but i'm never one to let others influence my viewings, in fact i'm more likely to watch something out of defiance!. Bullwhip had one thing going for me before the viewing anyway, the fact that Rhonda Fleming and those gorgeous eyes was in it had me interested right away. The picture isn't very good, and is in fact very morally dubious, all the characters are corrupt and shifty in one way shape or form, all motivated by greed or egocentric victories, this is all well and good if the surrounding film can do justice to a bunch of despicable people and create a taut climax shuddering picture. Sadly it doesn't</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>2</th>
      <td>&lt;s&gt;"Cavemen" exceeded my expectations, and not in a good way. It was even worse than I thought it would be. Basically, here's the show: The Cavemen are an alternate race, they face prejudice, etc. Quite possibly the stupidest idea ever created; almost being worthy of jail time for the writers. One show featured the cavemen going into a club, trying to pick up girls, and then nothing else happened. It was reminiscent of listening to a 22 minute Andy Rooney dialog, followed by death by steak knives via midget cannibals. For those who have not seen this show, here's an example of the dialog: "You're sure you're okay with going out with a caveman." "</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>3</th>
      <td>&lt;s&gt;This has got to be one of my very favorite Twilight Zone episodes, primarily for the portrayal of two lonely souls in a post-apocalyptic environment.&lt;br /&gt;&lt;br /&gt;The cobweb-strewn shops and rubble-laden streets are eerie in that particular way the Twilight Zone does best.&lt;br /&gt;&lt;br /&gt;While the parable can be a bit heavy-handed by today's drama standards, it is an excellent one - using the setting to make a statement relevant to the human experience, as well as geopolitics, in a way that is timeless. The entire drama rests solely on the shoulders of Mr. Bronson and Ms. Montgomery, who don't disappoint. (May they both rest in peace.)&lt;br /&gt;&lt;br</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>4</th>
      <td>&lt;s&gt;This is one of those movies that made me feel strongly for the need of making movies at all. Generally speaking, I am a fan of movies based on worthy true stories. And this one is GREAT! Besides Meryl's performance which has gained a lot of recognition and praise, the movie's greatest asset is the story it is based on. The riveting tale of a couple who suffer social and legal torture, after having undergone enormous emotional pain at the unexpected and brutal death of their infant child is really an eye-opening fable that exposes the inhumane side of fellow humans, and uncovers the barbarism of a very refined and lawful society. It is interesting to see how people who consider themselves as kind and intelligent people (the</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>5</th>
      <td>&lt;s&gt;Even though the book wasn't strictly accurate to the real situation it described it still carried a sense of Japan. I find it hard to believe that anyone who was involved in making this film had ever been to japan as it didn't feel Japanese in the slightest. Almost everything about it was terrible. I will admit the actors were generally quite good but couldn't stand a chance of saving it. Before the film started I was surprised that there were only ten people in the cinema on a Friday night shortly after the movie had opened in Japan. 30 minutes in I was amazed they stayed. I stayed so I would have the right to criticize it. The whole movie was punctuated my groans and suppressed laughs of disbelief from my Japanese girlfriend. Everyone</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>6</th>
      <td>&lt;s&gt;The first time I ever saw this movie was back in the 1980s as a wee lad. My dad actually recorded it off the TV. I must have watched is over 20 times, before the relatively recent release on DVD.&lt;br /&gt;&lt;br /&gt;I of course bought and watched the DVD and was taken aback by how much the dialogue had changed. In the first version, which I still have on VHS, the mood of the film, thanks to the dialogue, was actually very dark. However the new version, featuring Van Der Beek et al, is more comic.&lt;br /&gt;&lt;br /&gt;To put it another way, it's like watching the original US release of Akira with that dub, before watching the remastered version with the</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>7</th>
      <td>&lt;s&gt;This movie had good intentions and a good story to work with. The director and screenwriter of this movie failed miserably and created a dull, boring filmstrip that made me feel like I was back in Mr. Hartford's 8th grade Social Studies class -- way back in 67.&lt;br /&gt;&lt;br /&gt;What a waste, will somebody please take this story and make a real movie out of it - the story deserves it.&lt;br /&gt;&lt;br /&gt;Every time a scene had potential, all we were left with were a few clichés, combined with black and white footage that they probably got from The History Channel to show the action. Shameful.&lt;br /&gt;&lt;br /&gt;Ossie Davis was the only bright light in this dull fest</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>8</th>
      <td>&lt;s&gt;For those fans of Laurel and Hardy, the 1940s and beyond were a very sad time for the team. Their contracts with Hal Roach Studios had expired and now they were "free agents"--able to work for any studio who offered them a job. Unfortunately, Fox, RKO, MGM (without Roach) and even a French film company who hired the boys had absolutely no touch for their comedic talents. Plus, Stan and Ollie were a lot older and seeing these geriatric men taking pratfalls seemed sad, not particularly funny. Stan looked very ragged and Ollie's weight had ballooned up to the point where he could barely walk--and so it made me feel uncomfortable laughing at their very, very</td>
      <td>negative</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>9</th>
      <td>&lt;s&gt;Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie "Duty, Honor, Country" are not just mere words blathered from the lips of a high-brassed officer - it is the deep declaration of one man's total devotion to his country.&lt;br /&gt;&lt;</td>
      <td>positive</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Export">Export<a class="anchor-link" href="#Export">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # fastai model</span>
<span class="c1"># learn.save(f&#39;{model_name}_final&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # transformeres model</span>
<span class="c1"># Path(&#39;./models/transformers_model&#39;).mkdir(exist_ok=True)</span>
<span class="c1"># learn.model.save_pretrained(&#39;./models/transformers_model&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Path(&#39;./models/tokenizer&#39;).mkdir(exist_ok=True)</span>
<span class="c1"># tokenizer.save_pretrained(&#39;./models/tokenizer&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># transform_info = {&#39;category_map&#39;: list(learn.dbunch.vocab), &#39;max_len&#39;: max_len}</span>
<span class="c1"># transform_info = json.dumps(transform_info)</span>
<span class="c1"># Path(&#39;./models/transform_info.json&#39;).write_text(transform_info)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inference">Inference<a class="anchor-link" href="#Inference">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># inference_model = RobertaForSequenceClassification.from_pretrained(&#39;./models/transformers_model&#39;)</span>
<span class="c1"># inference_model.cuda()</span>
<span class="c1"># inference_model.eval()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dbch = get_databunch(dsrc, 64)</span>
<span class="c1"># for x, y in dbunch.train_dl:</span>
<span class="c1">#     pred, attention = inference_model(x)</span>
<span class="c1">#     pred = pred.argmax(-1)</span>
<span class="c1">#     print((pred == y).float().mean())</span>
<span class="c1">#     break</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Client-inference">Client inference<a class="anchor-link" href="#Client-inference">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># inference_model = RobertaForSequenceClassification.from_pretrained(&#39;./models/transformers_model&#39;)</span>
<span class="c1"># model.config.output_attentions = True</span>
<span class="c1"># inference_model.eval()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tokenizer = RobertaTokenizer.from_pretrained(&#39;./models/tokenizer&#39;)</span>
<span class="c1"># transform_info = json.loads(Path(&#39;./models/transform_info.json&#39;).read_text())</span>
<span class="c1"># category_map, max_len = transform_info[&#39;category_map&#39;], transform_info[&#39;max_len&#39;]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># test_sentences = [&#39;What a suck movie!!!&#39;, &#39;Feels like good. Nice movie.&#39;]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># with torch.no_grad():</span>
<span class="c1">#     for sentence in test_sentences:</span>
<span class="c1">#         numeric_sentence = tokenizer.encode(sentence)</span>
<span class="c1">#         x = torch.tensor([numeric_sentence])</span>
<span class="c1">#         pred, attention = inference_model(x)</span>
<span class="c1">#         pred = pred.argmax(-1)</span>
<span class="c1">#         print(category_map[pred])</span>
        
<span class="c1"># #         attention = attention[-1][0][-1][0]</span>
<span class="c1"># #         tok_sentence = tokenizer.convert_ids_to_tokens(numeric_sentence)</span>
<span class="c1"># #         print(tok_sentence, attention)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>
 

