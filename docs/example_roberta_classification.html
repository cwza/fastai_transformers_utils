---

title: Title

keywords: fastai
sidebar: home_sidebar

summary: "summary"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01_example_roberta_classification.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai_transformers_utils.general</span> <span class="k">import</span> <span class="n">TransformersTokenizer</span><span class="p">,</span> <span class="n">TransformersNumericalize</span><span class="p">,</span> <span class="n">Pad2Max</span><span class="p">,</span> <span class="n">BertSeqClassificationCallback</span><span class="p">,</span> <span class="n">roberta_SeqClassification_split</span>

<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastai2.basics</span> <span class="k">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastai2.text.all</span> <span class="k">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastai2.callback.all</span> <span class="k">import</span> <span class="o">*</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">RobertaForSequenceClassification</span><span class="p">,</span> <span class="n">RobertaTokenizer</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<p style="color: red;">
The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>
We recommend you <a href="https://www.tensorflow.org/guide/migrate" target="_blank">upgrade</a> now 
or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:
<a href="https://colab.research.google.com/notebooks/tensorflow_version.ipynb" target="_blank">more info</a>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># all_slow</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Integration-Test:-Roberta-Classification-on-IMDB_SAMPLE">Integration Test: Roberta Classification on IMDB_SAMPLE<a class="anchor-link" href="#Integration-Test:-Roberta-Classification-on-IMDB_SAMPLE">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_class</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># model_name = &#39;bert-base-uncased&#39;</span>
<span class="c1"># model_name = &#39;distilbert-base-uncased&#39;</span>
<span class="c1"># model_name = &#39;albert-base-v2&#39;</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;roberta-base&#39;</span>
<span class="n">max_len</span> <span class="o">=</span> <span class="mi">150</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Data-and-Tokenization">Data and Tokenization<a class="anchor-link" href="#Data-and-Tokenization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMDB_SAMPLE</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;texts.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>negative</td>
      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>positive</td>
      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>negative</td>
      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>positive</td>
      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie "Duty, Honor, Country" are not just mere words blathered from the lips of a high-brassed offic...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>negative</td>
      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_list</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">parallel_gen</span><span class="p">(</span><span class="n">TransformersTokenizer</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">sorted</span><span class="p">()</span><span class="o">.</span><span class="n">itemgot</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tok_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">tok_df</span><span class="o">.</span><span class="n">text</span> <span class="o">=</span>  <span class="n">tok_list</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># split tokens by &#39; &#39;</span>
<span class="n">tok_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>negative</td>
      <td>Un - ble eping - bel iev able ! ĠMeg ĠRyan Ġdoesn 't Ġeven Ġlook Ġher Ġusual Ġpert Ġl ovable Ġself Ġin Ġthis , Ġwhich Ġnormally Ġmakes Ġme Ġforgive Ġher Ġshallow Ġtick y Ġacting Ġsch tick . ĠHard Ġto Ġbelieve Ġshe Ġwas Ġthe Ġproducer Ġon Ġthis Ġdog . ĠPlus ĠKevin ĠK line : Ġwhat Ġkind Ġof Ġsuicide Ġtrip Ġhas Ġhis Ġcareer Ġbeen Ġon ? ĠWho osh ... ĠBan zai !!! ĠFinally Ġthis Ġwas Ġdirected Ġby Ġthe Ġguy Ġwho Ġdid ĠBig ĠChill ? ĠMust Ġbe Ġa Ġreplay Ġof ĠJon est own Ġ- Ġh ollywood Ġstyle . ĠWoo off f !</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>positive</td>
      <td>This Ġis Ġa Ġextremely Ġwell - made Ġfilm . ĠThe Ġacting , Ġscript Ġand Ġcamera - work Ġare Ġall Ġfirst - rate . ĠThe Ġmusic Ġis Ġgood , Ġtoo , Ġthough Ġit Ġis Ġmostly Ġearly Ġin Ġthe Ġfilm , Ġwhen Ġthings Ġare Ġstill Ġrelatively Ġche ery . ĠThere Ġare Ġno Ġreally Ġsuperst ars Ġin Ġthe Ġcast , Ġthough Ġseveral Ġfaces Ġwill Ġbe Ġfamiliar . ĠThe Ġentire Ġcast Ġdoes Ġan Ġexcellent Ġjob Ġwith Ġthe Ġscript .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; But Ġit Ġis Ġhard Ġto Ġwatch , Ġbecause Ġthere Ġis Ġno Ġgood Ġend Ġto Ġa Ġsituation Ġlike Ġthe Ġone Ġpresented . ĠIt Ġis Ġnow Ġfashionable Ġto Ġblame Ġthe ĠBritish Ġfor Ġse...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>negative</td>
      <td>Every Ġonce Ġin Ġa Ġlong Ġwhile Ġa Ġmovie Ġwill Ġcome Ġalong Ġthat Ġwill Ġbe Ġso Ġawful Ġthat ĠI Ġfeel Ġcompelled Ġto Ġwarn Ġpeople . ĠIf ĠI Ġlabor Ġall Ġmy Ġdays Ġand ĠI Ġcan Ġsave Ġbut Ġone Ġsoul Ġfrom Ġwatching Ġthis Ġmovie , Ġhow Ġgreat Ġwill Ġbe Ġmy Ġjoy .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; Where Ġto Ġbegin Ġmy Ġdiscussion Ġof Ġpain . ĠFor Ġstarters , Ġthere Ġwas Ġa Ġmusical Ġmont age Ġevery Ġfive Ġminutes . ĠThere Ġwas Ġno Ġcharacter Ġdevelopment . ĠEvery Ġcharacter Ġwas Ġa Ġstereotype . ĠWe Ġhad Ġswearing Ġguy , Ġfat Ġguy Ġwho Ġeats Ġdon uts , Ġgoofy Ġforeign Ġguy , Ġetc . ĠThe Ġscript Ġfelt Ġas Ġif ...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>positive</td>
      <td>Name Ġjust Ġsays Ġit Ġall . ĠI Ġwatched Ġthis Ġmovie Ġwith Ġmy Ġdad Ġwhen Ġit Ġcame Ġout Ġand Ġhaving Ġserved Ġin ĠKorea Ġhe Ġhad Ġgreat Ġadmiration Ġfor Ġthe Ġman . ĠThe Ġdisappointing Ġthing Ġabout Ġthis Ġfilm Ġis Ġthat Ġit Ġonly Ġconcentrate Ġon Ġa Ġshort Ġperiod Ġof Ġthe Ġman 's Ġlife Ġ- Ġinterestingly Ġenough Ġthe Ġman 's Ġentire Ġlife Ġwould Ġhave Ġmade Ġsuch Ġan Ġepic Ġbio - pic Ġthat Ġit Ġis Ġstaggering Ġto Ġimagine Ġthe Ġcost Ġfor Ġproduction .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; Some Ġposters Ġel ude Ġto Ġthe Ġflawed Ġcharacteristics Ġabout Ġthe Ġman , Ġwhich Ġare Ġcheap Ġshots . ĠThe Ġtheme Ġof Ġt...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>negative</td>
      <td>This Ġmovie Ġsucceeds Ġat Ġbeing Ġone Ġof Ġthe Ġmost Ġunique Ġmovies Ġyou 've Ġseen . ĠHowever Ġthis Ġcomes Ġfrom Ġthe Ġfact Ġthat Ġyou Ġcan 't Ġmake Ġheads Ġor Ġtails Ġof Ġthis Ġmess . ĠIt Ġalmost Ġseems Ġas Ġa Ġseries Ġof Ġchallenges Ġset Ġup Ġto Ġdetermine Ġwhether Ġor Ġnot Ġyou Ġare Ġwilling Ġto Ġwalk Ġout Ġof Ġthe Ġmovie Ġand Ġgive Ġup Ġthe Ġmoney Ġyou Ġjust Ġpaid . ĠIf Ġyou Ġdon 't Ġwant Ġto Ġfeel Ġslight ed Ġyou 'll Ġsit Ġthrough Ġthis Ġhorrible Ġfilm Ġand Ġdevelop Ġa Ġreal Ġsense Ġof Ġpity Ġfor Ġthe Ġactors Ġinvolved , Ġthey 've Ġall Ġseen Ġbetter Ġdays , Ġbut Ġthen Ġyou Ġrealize Ġ...</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tok_df.to_csv(f&#39;{model_name}_tok.csv&#39;, index=False)</span>
<span class="c1"># tok_df = pd.read_csv(f&#39;{model_name}_tok.csv&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Databunch">Databunch<a class="anchor-link" href="#Databunch">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">splits</span> <span class="o">=</span> <span class="n">ColSplitter</span><span class="p">()(</span><span class="n">tok_df</span><span class="p">)</span>
<span class="n">ds_tfms</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">attrgetter</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">),</span> <span class="n">TransformersNumericalize</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),</span> <span class="n">Pad2Max</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)],</span> 
    <span class="p">[</span><span class="n">attrgetter</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">),</span> <span class="n">Categorize</span><span class="p">()]</span>
<span class="p">]</span>
<span class="n">dsrc</span> <span class="o">=</span> <span class="n">DataSource</span><span class="p">(</span><span class="n">tok_df</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">ds_tfms</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">)</span>
<span class="n">dsrc</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dsrc</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">dsrc</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((tensor([    0,  9685,    12,  5225, 24320,    12,  8494, 18421,   868,   328,
          14938,  1774,   630,    75,   190,   356,    69,  4505, 32819,   784,
          30289,  1403,    11,    42,     6,    61,  6329,   817,   162, 20184,
             69, 16762, 10457,   219,  3501,  8447, 41791,     4,  6206,     7,
            679,    79,    21,     5,  3436,    15,    42,  2335,     4,  4642,
           2363,   229,  1902,    35,    99,   761,     9,  4260,  1805,    34,
             39,   756,    57,    15,   116,  3394,  5212,   734,  5981, 23642,
          16506,  3347,    42,    21,  3660,    30,     5,  2173,    54,   222,
           1776, 25928,   116,  8495,    28,    10, 16462,     9,  4160,   990,
           3355,   111,  1368,  9718,  2496,     4, 29935,  1529,   506,   328,
              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),
  tensor(0)),
 (&#34;&lt;s&gt;Un-bleeping-believable! Meg Ryan doesn&#39;t even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!&lt;/s&gt;&#34;,
  &#39;negative&#39;))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_databunch</span><span class="p">(</span><span class="n">dsrc</span><span class="p">,</span> <span class="n">bs</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dsrc</span><span class="o">.</span><span class="n">databunch</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">after_batch</span><span class="o">=</span><span class="p">[</span><span class="n">Cuda</span><span class="p">()])</span>

<span class="n">dbunch</span> <span class="o">=</span> <span class="n">get_databunch</span><span class="p">(</span><span class="n">dsrc</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># for x in dbunch.train_dl:</span>
<span class="c1">#     print(x[0].shape, x[0].dtype)</span>
<span class="c1">#     print(x[1].shape, x[1].dtype)</span>
<span class="c1">#     break</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dbunch</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>&lt;s&gt;The views of Earth that are claimed in this film to have been faked by NASA have recently been compared with the historical weather data for the time of Apollo 11, and show a good match between the cloud patterns in the video sequence and the actual rainfall records on the day.&lt;br /&gt;&lt;br /&gt;This would seem to undermine the entire argument put forward in the film that the "whole Earth" picture is actually a small part of the planet framed by the spacecraft window.&lt;br /&gt;&lt;br /&gt;I am waiting for Bart Sibrel to now claim that the historical weather data has been faked by NASA, though that would no doubt involve them in also replacing every archived newspaper copy with a weather map, and the ones in private hands would still be</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>1</th>
      <td>&lt;s&gt;With its companion piece MASTERS OF HORROR, NIGHTMARES AND DREAMSCAPES can only be seen as the absolute nadir of the genre that began so auspiciously with THE TWILIGHT ZONE and THE OUTER LIMITS.&lt;br /&gt;&lt;br /&gt;Of course, part of the problem is that it does nothing to be of any interest to a comparatively adult audience, instead aiming at TEN-YEAR-OLDS, who are only able to count body-bags, and scarcely that. And so grossness is king, and King is grossness.&lt;br /&gt;&lt;br /&gt;Stephen King is simply illiterate  in general he has the aptitude for storytelling of Bart Simpson. Since he cannot</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learner-and-Train">Learner and Train<a class="anchor-link" href="#Learner-and-Train">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dbunch</span> <span class="o">=</span> <span class="n">get_databunch</span><span class="p">(</span><span class="n">dsrc</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="o">=</span><span class="n">num_class</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dbunch</span><span class="p">,</span> 
                <span class="n">model</span><span class="p">,</span> 
                <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span> 
                <span class="n">opt_func</span><span class="o">=</span><span class="n">ranger</span><span class="p">,</span>
                <span class="n">splitter</span><span class="o">=</span><span class="n">roberta_SeqClassification_split</span><span class="p">,</span> 
                <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">BertSeqClassificationCallback</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)],</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">],</span>
               <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>RobertaForSequenceClassification (Input shape: 64 x 150)
================================================================
Layer (type)         Output Shape         Param #    Trainable 
================================================================
Embedding            64 x 150 x 768       38,603,520 False     
________________________________________________________________
Embedding            64 x 150 x 768       394,752    False     
________________________________________________________________
Embedding            64 x 150 x 768       768        False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 768             590,592    False     
________________________________________________________________
Tanh                 64 x 768             0          False     
________________________________________________________________
Linear               64 x 768             590,592    True      
________________________________________________________________
Dropout              64 x 768             0          False     
________________________________________________________________
Linear               64 x 2               1,538      True      
________________________________________________________________

Total params: 125,237,762
Total trainable params: 592,130
Total non-trainable params: 124,645,632

Optimizer used: &lt;function ranger at 0x7f04b328fd90&gt;
Loss function: FlattenedLoss of CrossEntropyLoss()

Model frozen up to parameter group number 14

Callbacks:
  - TrainEvalCallback
  - Recorder
  - ProgressCallback
  - BertSeqClassificationCallback</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.lr_find()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">0.8</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.693394</td>
      <td>0.685471</td>
      <td>0.535000</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.fit_one_cycle(5, 1e-2, moms=(0.8,0.7,0.8))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.freeze_to(-3)</span>
<span class="c1"># learn.fit_one_cycle(5, 1e-3, moms=(0.8,0.7,0.8))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># interp = Interpretation.from_learner(learn)</span>
<span class="c1"># interp.plot_top_losses(6)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Export">Export<a class="anchor-link" href="#Export">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # fastai model</span>
<span class="c1"># learn.save(f&#39;{model_name}_final&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # transformeres model</span>
<span class="c1"># Path(&#39;./models/transformers_model&#39;).mkdir(exist_ok=True)</span>
<span class="c1"># learn.model.save_pretrained(&#39;./models/transformers_model&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Path(&#39;./models/tokenizer&#39;).mkdir(exist_ok=True)</span>
<span class="c1"># tokenizer.save_pretrained(&#39;./models/tokenizer&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># transform_info = {&#39;category_map&#39;: list(learn.dbunch.vocab), &#39;max_len&#39;: max_len}</span>
<span class="c1"># transform_info = json.dumps(transform_info)</span>
<span class="c1"># Path(&#39;./models/transform_info.json&#39;).write_text(transform_info)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inference">Inference<a class="anchor-link" href="#Inference">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># inference_model = RobertaForSequenceClassification.from_pretrained(&#39;./models/transformers_model&#39;)</span>
<span class="c1"># inference_model.cuda()</span>
<span class="c1"># inference_model.eval()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dbch = get_databunch(dsrc, 64)</span>
<span class="c1"># for x, y in dbunch.train_dl:</span>
<span class="c1">#     pred, attention = inference_model(x)</span>
<span class="c1">#     pred = pred.argmax(-1)</span>
<span class="c1">#     print((pred == y).float().mean())</span>
<span class="c1">#     break</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Client-inference">Client inference<a class="anchor-link" href="#Client-inference">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># inference_model = RobertaForSequenceClassification.from_pretrained(&#39;./models/transformers_model&#39;)</span>
<span class="c1"># model.config.output_attentions = True</span>
<span class="c1"># inference_model.eval()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tokenizer = RobertaTokenizer.from_pretrained(&#39;./models/tokenizer&#39;)</span>
<span class="c1"># transform_info = json.loads(Path(&#39;./models/transform_info.json&#39;).read_text())</span>
<span class="c1"># category_map, max_len = transform_info[&#39;category_map&#39;], transform_info[&#39;max_len&#39;]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># test_sentences = [&#39;What a suck movie!!!&#39;, &#39;Feels like good. Nice movie.&#39;]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># with torch.no_grad():</span>
<span class="c1">#     for sentence in test_sentences:</span>
<span class="c1">#         numeric_sentence = tokenizer.encode(sentence)</span>
<span class="c1">#         x = torch.tensor([numeric_sentence])</span>
<span class="c1">#         pred, attention = inference_model(x)</span>
<span class="c1">#         pred = pred.argmax(-1)</span>
<span class="c1">#         print(category_map[pred])</span>
        
<span class="c1"># #         attention = attention[-1][0][-1][0]</span>
<span class="c1"># #         tok_sentence = tokenizer.convert_ids_to_tokens(numeric_sentence)</span>
<span class="c1"># #         print(tok_sentence, attention)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>
 

