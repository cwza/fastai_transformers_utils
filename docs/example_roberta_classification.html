---

title: Integration Test: Roberta Classification on IMDB_SAMPLE

keywords: fastai
sidebar: home_sidebar


---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01_example_roberta_classification.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">capture</span>
from fastai_transformers_utils.general import TransformersTokenizer, TransformersNumericalize, Pad2Max, BertSeqClassificationCallback, roberta_SeqClassification_split

import json
from typing import *
from fastai2.basics import *
from fastai2.text.all import *
from fastai2.callback.all import *

from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification, RobertaTokenizer
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># all_slow</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_class</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># model_name = &#39;bert-base-uncased&#39;</span>
<span class="c1"># model_name = &#39;distilbert-base-uncased&#39;</span>
<span class="c1"># model_name = &#39;albert-base-v2&#39;</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;roberta-base&#39;</span>
<span class="n">max_len</span> <span class="o">=</span> <span class="mi">150</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Data-and-Tokenization">Data and Tokenization<a class="anchor-link" href="#Data-and-Tokenization">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMDB_SAMPLE</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;texts.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>negative</td>
      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>positive</td>
      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>negative</td>
      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>positive</td>
      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie "Duty, Honor, Country" are not just mere words blathered from the lips of a high-brassed offic...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>negative</td>
      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok_list</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">parallel_gen</span><span class="p">(</span><span class="n">TransformersTokenizer</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">sorted</span><span class="p">()</span><span class="o">.</span><span class="n">itemgot</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tok_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">tok_df</span><span class="o">.</span><span class="n">text</span> <span class="o">=</span>  <span class="n">tok_list</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># split tokens by &#39; &#39;</span>
<span class="n">tok_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>is_valid</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>negative</td>
      <td>Un - ble eping - bel iev able ! ĠMeg ĠRyan Ġdoesn 't Ġeven Ġlook Ġher Ġusual Ġpert Ġl ovable Ġself Ġin Ġthis , Ġwhich Ġnormally Ġmakes Ġme Ġforgive Ġher Ġshallow Ġtick y Ġacting Ġsch tick . ĠHard Ġto Ġbelieve Ġshe Ġwas Ġthe Ġproducer Ġon Ġthis Ġdog . ĠPlus ĠKevin ĠK line : Ġwhat Ġkind Ġof Ġsuicide Ġtrip Ġhas Ġhis Ġcareer Ġbeen Ġon ? ĠWho osh ... ĠBan zai !!! ĠFinally Ġthis Ġwas Ġdirected Ġby Ġthe Ġguy Ġwho Ġdid ĠBig ĠChill ? ĠMust Ġbe Ġa Ġreplay Ġof ĠJon est own Ġ- Ġh ollywood Ġstyle . ĠWoo off f !</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>positive</td>
      <td>This Ġis Ġa Ġextremely Ġwell - made Ġfilm . ĠThe Ġacting , Ġscript Ġand Ġcamera - work Ġare Ġall Ġfirst - rate . ĠThe Ġmusic Ġis Ġgood , Ġtoo , Ġthough Ġit Ġis Ġmostly Ġearly Ġin Ġthe Ġfilm , Ġwhen Ġthings Ġare Ġstill Ġrelatively Ġche ery . ĠThere Ġare Ġno Ġreally Ġsuperst ars Ġin Ġthe Ġcast , Ġthough Ġseveral Ġfaces Ġwill Ġbe Ġfamiliar . ĠThe Ġentire Ġcast Ġdoes Ġan Ġexcellent Ġjob Ġwith Ġthe Ġscript .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; But Ġit Ġis Ġhard Ġto Ġwatch , Ġbecause Ġthere Ġis Ġno Ġgood Ġend Ġto Ġa Ġsituation Ġlike Ġthe Ġone Ġpresented . ĠIt Ġis Ġnow Ġfashionable Ġto Ġblame Ġthe ĠBritish Ġfor Ġse...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>negative</td>
      <td>Every Ġonce Ġin Ġa Ġlong Ġwhile Ġa Ġmovie Ġwill Ġcome Ġalong Ġthat Ġwill Ġbe Ġso Ġawful Ġthat ĠI Ġfeel Ġcompelled Ġto Ġwarn Ġpeople . ĠIf ĠI Ġlabor Ġall Ġmy Ġdays Ġand ĠI Ġcan Ġsave Ġbut Ġone Ġsoul Ġfrom Ġwatching Ġthis Ġmovie , Ġhow Ġgreat Ġwill Ġbe Ġmy Ġjoy .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; Where Ġto Ġbegin Ġmy Ġdiscussion Ġof Ġpain . ĠFor Ġstarters , Ġthere Ġwas Ġa Ġmusical Ġmont age Ġevery Ġfive Ġminutes . ĠThere Ġwas Ġno Ġcharacter Ġdevelopment . ĠEvery Ġcharacter Ġwas Ġa Ġstereotype . ĠWe Ġhad Ġswearing Ġguy , Ġfat Ġguy Ġwho Ġeats Ġdon uts , Ġgoofy Ġforeign Ġguy , Ġetc . ĠThe Ġscript Ġfelt Ġas Ġif ...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>positive</td>
      <td>Name Ġjust Ġsays Ġit Ġall . ĠI Ġwatched Ġthis Ġmovie Ġwith Ġmy Ġdad Ġwhen Ġit Ġcame Ġout Ġand Ġhaving Ġserved Ġin ĠKorea Ġhe Ġhad Ġgreat Ġadmiration Ġfor Ġthe Ġman . ĠThe Ġdisappointing Ġthing Ġabout Ġthis Ġfilm Ġis Ġthat Ġit Ġonly Ġconcentrate Ġon Ġa Ġshort Ġperiod Ġof Ġthe Ġman 's Ġlife Ġ- Ġinterestingly Ġenough Ġthe Ġman 's Ġentire Ġlife Ġwould Ġhave Ġmade Ġsuch Ġan Ġepic Ġbio - pic Ġthat Ġit Ġis Ġstaggering Ġto Ġimagine Ġthe Ġcost Ġfor Ġproduction .&lt; br Ġ/ &gt;&lt; br Ġ/&gt; Some Ġposters Ġel ude Ġto Ġthe Ġflawed Ġcharacteristics Ġabout Ġthe Ġman , Ġwhich Ġare Ġcheap Ġshots . ĠThe Ġtheme Ġof Ġt...</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>negative</td>
      <td>This Ġmovie Ġsucceeds Ġat Ġbeing Ġone Ġof Ġthe Ġmost Ġunique Ġmovies Ġyou 've Ġseen . ĠHowever Ġthis Ġcomes Ġfrom Ġthe Ġfact Ġthat Ġyou Ġcan 't Ġmake Ġheads Ġor Ġtails Ġof Ġthis Ġmess . ĠIt Ġalmost Ġseems Ġas Ġa Ġseries Ġof Ġchallenges Ġset Ġup Ġto Ġdetermine Ġwhether Ġor Ġnot Ġyou Ġare Ġwilling Ġto Ġwalk Ġout Ġof Ġthe Ġmovie Ġand Ġgive Ġup Ġthe Ġmoney Ġyou Ġjust Ġpaid . ĠIf Ġyou Ġdon 't Ġwant Ġto Ġfeel Ġslight ed Ġyou 'll Ġsit Ġthrough Ġthis Ġhorrible Ġfilm Ġand Ġdevelop Ġa Ġreal Ġsense Ġof Ġpity Ġfor Ġthe Ġactors Ġinvolved , Ġthey 've Ġall Ġseen Ġbetter Ġdays , Ġbut Ġthen Ġyou Ġrealize Ġ...</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tok_df.to_csv(f&#39;{model_name}_tok.csv&#39;, index=False)</span>
<span class="c1"># tok_df = pd.read_csv(f&#39;{model_name}_tok.csv&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Databunch">Databunch<a class="anchor-link" href="#Databunch">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">splits</span> <span class="o">=</span> <span class="n">ColSplitter</span><span class="p">()(</span><span class="n">tok_df</span><span class="p">)</span>
<span class="n">ds_tfms</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">attrgetter</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">),</span> <span class="n">TransformersNumericalize</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),</span> <span class="n">Pad2Max</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)],</span> 
    <span class="p">[</span><span class="n">attrgetter</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">),</span> <span class="n">Categorize</span><span class="p">()]</span>
<span class="p">]</span>
<span class="n">dsrc</span> <span class="o">=</span> <span class="n">DataSource</span><span class="p">(</span><span class="n">tok_df</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="n">ds_tfms</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">)</span>
<span class="n">dsrc</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dsrc</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">dsrc</span><span class="o">.</span><span class="n">train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((tensor([    0,  9685,    12,  5225, 24320,    12,  8494, 18421,   868,   328,
          14938,  1774,   630,    75,   190,   356,    69,  4505, 32819,   784,
          30289,  1403,    11,    42,     6,    61,  6329,   817,   162, 20184,
             69, 16762, 10457,   219,  3501,  8447, 41791,     4,  6206,     7,
            679,    79,    21,     5,  3436,    15,    42,  2335,     4,  4642,
           2363,   229,  1902,    35,    99,   761,     9,  4260,  1805,    34,
             39,   756,    57,    15,   116,  3394,  5212,   734,  5981, 23642,
          16506,  3347,    42,    21,  3660,    30,     5,  2173,    54,   222,
           1776, 25928,   116,  8495,    28,    10, 16462,     9,  4160,   990,
           3355,   111,  1368,  9718,  2496,     4, 29935,  1529,   506,   328,
              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
              1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),
  tensor(0)),
 (&#34;&lt;s&gt;Un-bleeping-believable! Meg Ryan doesn&#39;t even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!&lt;/s&gt;&#34;,
  &#39;negative&#39;))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_databunch</span><span class="p">(</span><span class="n">dsrc</span><span class="p">,</span> <span class="n">bs</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dsrc</span><span class="o">.</span><span class="n">databunch</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">after_batch</span><span class="o">=</span><span class="p">[</span><span class="n">Cuda</span><span class="p">()])</span>

<span class="n">dbunch</span> <span class="o">=</span> <span class="n">get_databunch</span><span class="p">(</span><span class="n">dsrc</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># for x in dbunch.train_dl:</span>
<span class="c1">#     print(x[0].shape, x[0].dtype)</span>
<span class="c1">#     print(x[1].shape, x[1].dtype)</span>
<span class="c1">#     break</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dbunch</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>&lt;s&gt;First of all, in defense of JOAN FONTAINE, it must be said that Ginger Rogers would have been terribly miscast as Alyce, the young British lady who has the title role. Fontaine makes a fetching picture as the heroine here, but her acting inexperience shows badly and her dancing is better left unmentioned. Fortunately, she went on to better things.&lt;br /&gt;&lt;br /&gt;But here it's FRED ASTAIRE, GEORGE BURNS and GRACIE ALLEN who get the top billing--and they are excellent. Fans of Burns &amp; Allen will be surprised at how easily they fit into Astaire's dance routines. Especially interesting is the big fun house routine that won choreographer Hermes Pans</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>&lt;s&gt;There are no people like "Show People" Marion Davies (as Peggy Pepper) and William Haines (as Billy Boone). My introduction to Ms. Davies was a "clip" from this film; the delightfully spoofy one in which she lowers a scarf to reveal different emotions. My introduction to Mr. Haines was in viewing this film, presently; though, it's possible I've seen him in a less memorable role. Haines makes an incredible impression, when he joins Davies for a commissary meal - tossing his hat into the ring with some wonderful bits at the dining table. Indeed, Haines and Davies deliver great comic performances.&lt;br /&gt;&lt;br /&gt;The story starts off with Dell Henderson (Col</td>
      <td>positive</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learner-and-Train">Learner and Train<a class="anchor-link" href="#Learner-and-Train">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dbunch</span> <span class="o">=</span> <span class="n">get_databunch</span><span class="p">(</span><span class="n">dsrc</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="o">=</span><span class="n">num_class</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dbunch</span><span class="p">,</span> 
                <span class="n">model</span><span class="p">,</span> 
                <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span> 
                <span class="n">opt_func</span><span class="o">=</span><span class="n">ranger</span><span class="p">,</span>
                <span class="n">splitter</span><span class="o">=</span><span class="n">roberta_SeqClassification_split</span><span class="p">,</span> 
                <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="n">BertSeqClassificationCallback</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)],</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">],</span>
               <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>RobertaForSequenceClassification (Input shape: 64 x 150)
================================================================
Layer (type)         Output Shape         Param #    Trainable 
================================================================
Embedding            64 x 150 x 768       38,603,520 False     
________________________________________________________________
Embedding            64 x 150 x 768       394,752    False     
________________________________________________________________
Embedding            64 x 150 x 768       768        False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
Dropout              64 x 12 x 150 x 150  0          False     
________________________________________________________________
Linear               64 x 150 x 768       590,592    False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 150 x 3072      2,362,368  False     
________________________________________________________________
Linear               64 x 150 x 768       2,360,064  False     
________________________________________________________________
LayerNorm            64 x 150 x 768       1,536      False     
________________________________________________________________
Dropout              64 x 150 x 768       0          False     
________________________________________________________________
Linear               64 x 768             590,592    False     
________________________________________________________________
Tanh                 64 x 768             0          False     
________________________________________________________________
Linear               64 x 768             590,592    True      
________________________________________________________________
Dropout              64 x 768             0          False     
________________________________________________________________
Linear               64 x 2               1,538      True      
________________________________________________________________

Total params: 125,237,762
Total trainable params: 592,130
Total non-trainable params: 124,645,632

Optimizer used: &lt;function ranger at 0x7f602227da60&gt;
Loss function: FlattenedLoss of CrossEntropyLoss()

Model frozen up to parameter group number 14

Callbacks:
  - TrainEvalCallback
  - Recorder
  - ProgressCallback
  - BertSeqClassificationCallback</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.lr_find()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">moms</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="mf">0.8</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.692910</td>
      <td>0.688130</td>
      <td>0.535000</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.fit_one_cycle(5, 1e-2, moms=(0.8,0.7,0.8))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># learn.freeze_to(-3)</span>
<span class="c1"># learn.fit_one_cycle(5, 1e-3, moms=(0.8,0.7,0.8))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># interp = Interpretation.from_learner(learn)</span>
<span class="c1"># interp.plot_top_losses(6)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Export">Export<a class="anchor-link" href="#Export">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # fastai model</span>
<span class="c1"># learn.save(f&#39;{model_name}_final&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # transformeres model</span>
<span class="c1"># Path(&#39;./models/transformers_model&#39;).mkdir(exist_ok=True)</span>
<span class="c1"># learn.model.save_pretrained(&#39;./models/transformers_model&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Path(&#39;./models/tokenizer&#39;).mkdir(exist_ok=True)</span>
<span class="c1"># tokenizer.save_pretrained(&#39;./models/tokenizer&#39;)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># transform_info = {&#39;category_map&#39;: list(learn.dbunch.vocab), &#39;max_len&#39;: max_len}</span>
<span class="c1"># transform_info = json.dumps(transform_info)</span>
<span class="c1"># Path(&#39;./models/transform_info.json&#39;).write_text(transform_info)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inference">Inference<a class="anchor-link" href="#Inference">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># inference_model = RobertaForSequenceClassification.from_pretrained(&#39;./models/transformers_model&#39;)</span>
<span class="c1"># inference_model.cuda()</span>
<span class="c1"># inference_model.eval()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dbch = get_databunch(dsrc, 64)</span>
<span class="c1"># for x, y in dbunch.train_dl:</span>
<span class="c1">#     pred, attention = inference_model(x)</span>
<span class="c1">#     pred = pred.argmax(-1)</span>
<span class="c1">#     print((pred == y).float().mean())</span>
<span class="c1">#     break</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Client-inference">Client inference<a class="anchor-link" href="#Client-inference">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># inference_model = RobertaForSequenceClassification.from_pretrained(&#39;./models/transformers_model&#39;)</span>
<span class="c1"># model.config.output_attentions = True</span>
<span class="c1"># inference_model.eval()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tokenizer = RobertaTokenizer.from_pretrained(&#39;./models/tokenizer&#39;)</span>
<span class="c1"># transform_info = json.loads(Path(&#39;./models/transform_info.json&#39;).read_text())</span>
<span class="c1"># category_map, max_len = transform_info[&#39;category_map&#39;], transform_info[&#39;max_len&#39;]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># test_sentences = [&#39;What a suck movie!!!&#39;, &#39;Feels like good. Nice movie.&#39;]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># with torch.no_grad():</span>
<span class="c1">#     for sentence in test_sentences:</span>
<span class="c1">#         numeric_sentence = tokenizer.encode(sentence)</span>
<span class="c1">#         x = torch.tensor([numeric_sentence])</span>
<span class="c1">#         pred, attention = inference_model(x)</span>
<span class="c1">#         pred = pred.argmax(-1)</span>
<span class="c1">#         print(category_map[pred])</span>
        
<span class="c1"># #         attention = attention[-1][0][-1][0]</span>
<span class="c1"># #         tok_sentence = tokenizer.convert_ids_to_tokens(numeric_sentence)</span>
<span class="c1"># #         print(tok_sentence, attention)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>
 

